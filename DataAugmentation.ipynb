{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qxKK4_OWI-B"
      },
      "source": [
        "# imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ-wCyLLluvf",
        "outputId": "c985c99d-a5b3-47d9-d479-dc1bc5a14402"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googletrans==3.1.0a0 in c:\\users\\cptgu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in c:\\users\\cptgu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: certifi in c:\\users\\cptgu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2023.7.22)\n",
            "Requirement already satisfied: hstspreload in c:\\users\\cptgu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.10.1)\n",
            "Requirement already satisfied: sniffio in c:\\users\\cptgu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in c:\\users\\cptgu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in c:\\users\\cptgu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\cptgu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in c:\\users\\cptgu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\cptgu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in c:\\users\\cptgu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\cptgu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\cptgu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install googletrans==3.1.0a0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pimr4vAEWHKE",
        "outputId": "0fe9622e-63e9-48a1-8a9f-a16a45b4ba81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\cptgu\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\cptgu\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\cptgu\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from googletrans import Translator\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FOK6y0NWVhd"
      },
      "source": [
        "# Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfzWxkxvWk0e",
        "outputId": "1cae19bf-6ae6-4e7b-a9a3-ce3871b0e916"
      },
      "outputs": [
        {
          "ename": "UnicodeEncodeError",
          "evalue": "'charmap' codec can't encode character '\\u014d' in position 5: character maps to <undefined>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 56\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\cptgu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u014d' in position 5: character maps to <undefined>"
          ]
        }
      ],
      "source": [
        "# read the training file and extract genre and plot\n",
        "\n",
        "input_file = 'train.txt'\n",
        "output_file = 'augmented.txt'\n",
        "genre_plot = []\n",
        "genres = []\n",
        "plots = []\n",
        "translator = Translator()\n",
        "drama_counter = 0\n",
        "line_counter = 0\n",
        "\n",
        "with open(input_file, 'r', encoding='utf-8') as infile:\n",
        "    for line in infile:\n",
        "        line_counter += 1\n",
        "        if line_counter % 100 == 0:\n",
        "          print(line_counter)\n",
        "        # Split the line by tabs\n",
        "        parts = line.split('\\t')\n",
        "        plot = parts[4]\n",
        "\n",
        "        if parts[2] == 'sci-fi':\n",
        "            # Perform back translation\n",
        "            translated = translator.translate(plot, src='en', dest='es').text\n",
        "            back_translated = translator.translate(translated, src='es', dest='en').text\n",
        "\n",
        "            # Perform synonym replacement\n",
        "            translated2 = translator.translate(plot, src='en', dest='fr').text\n",
        "            back_translated2 = translator.translate(translated2, src='fr', dest='en').text\n",
        "\n",
        "            # Append both plots to the specified file\n",
        "            with open(output_file, 'a') as f:\n",
        "                f.write(line)\n",
        "                f.write(parts[0] + '\\t' + parts[1] + '\\t' + parts[2] + '\\t' + parts[3] + '\\t' + back_translated + '\\n')\n",
        "                f.write(parts[0] + '\\t' + parts[1] + '\\t' + parts[2] + '\\t' + parts[3] + '\\t' + back_translated2 + '\\n')\n",
        "\n",
        "\n",
        "        elif parts[2] in {'crime', 'animation'}:\n",
        "            # Perform back translation\n",
        "            translated = translator.translate(plot, src='en', dest='es').text\n",
        "            back_translated = translator.translate(translated, src='es', dest='en').text\n",
        "\n",
        "            # Append both plots to the specified file\n",
        "            with open(output_file, 'a') as f:\n",
        "                f.write(line)\n",
        "                f.write(parts[0] + '\\t' + parts[1] + '\\t' + parts[2] + '\\t' + parts[3] + '\\t' + back_translated + '\\n')\n",
        "\n",
        "\n",
        "        elif (parts[2] == 'drama'):\n",
        "            if (drama_counter < 1000):\n",
        "                drama_counter += 1\n",
        "                with open(output_file, 'a') as f:\n",
        "                    f.write(line)\n",
        "\n",
        "        else:\n",
        "            with open(output_file, 'a') as f:\n",
        "                f.write(line)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj5IF4uDWqoh"
      },
      "source": [
        "# Read from new file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Vqot_7rcVevM",
        "outputId": "c08c815b-c176-44a9-b9eb-084832e65930"
      },
      "outputs": [],
      "source": [
        "# read the training file and extract genre and plot\n",
        "\n",
        "input_file = '/content/sample_data/augmented.txt'\n",
        "genre_plot = []\n",
        "genres = []\n",
        "plots = []\n",
        "counter = 0\n",
        "\n",
        "with open(input_file, 'r', encoding='utf-8') as infile:\n",
        "    for line in infile:\n",
        "        counter += 1\n",
        "        print (counter)\n",
        "        # Split the line by tabs\n",
        "        parts = line.split('\\t')\n",
        "\n",
        "        # Extract genre and plot\n",
        "        genres.append(parts[2])\n",
        "        plots.append(parts[4])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L954u-zIXlRg"
      },
      "source": [
        "# Balance test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdXcFWvtXnV3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Count the frequency of each genre using Counter\n",
        "genre_counts = Counter(genres)\n",
        "\n",
        "# Extract genre names and their respective counts\n",
        "labels = list(genre_counts.keys())\n",
        "counts = list(genre_counts.values())\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(labels, counts, color='skyblue')\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Genre Frequency', fontsize=14)\n",
        "plt.xlabel('Genre', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
