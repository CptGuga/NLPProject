{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxeWqldmQEVy"
      },
      "source": [
        "# 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bLzK9CmTSZD",
        "outputId": "8ca96ffb-0a58-4d89-bf6a-fac8cd992302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.10/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.8.30)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.10.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n"
          ]
        }
      ],
      "source": [
        "pip install googletrans==3.1.0a0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "id": "wbdCVvp-HZAl",
        "outputId": "c5301509-5833-436f-a692-df2e5c07b731",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzP2GQu2QDe0",
        "outputId": "e35e0539-d924-4fc1-8832-b1870eb7a529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from googletrans import Translator\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F56d1tcQQJm"
      },
      "source": [
        "# 2. Data set load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bdxR0dUBQSar"
      },
      "outputs": [],
      "source": [
        "# read the training file and extract genre and plot\n",
        "\n",
        "input_file = '/content/sample_data/train.txt'\n",
        "\n",
        "genres = []\n",
        "plots = []\n",
        "\n",
        "with open(input_file, 'r', encoding='utf-8') as infile:\n",
        "    for line in infile:\n",
        "        # Split the line by tabs\n",
        "        parts = line.split('\\t')\n",
        "\n",
        "        # Extract genre and plot\n",
        "        genres.append(parts[2])\n",
        "        plots.append(parts[4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX5FQg6YQVPC"
      },
      "source": [
        "# 3. Train test split and Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxqicGeJQa7X",
        "outputId": "b202fd5b-d1e1-4d56-81ef-57329d40b01e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, Z_train, Z_test = train_test_split(plots, genres, test_size=0.1, random_state=33)\n",
        "\n",
        "translator = Translator()\n",
        "aug_X_train = []\n",
        "aug_Z_train = []\n",
        "drama_counter = 0\n",
        "counter = 0\n",
        "for i in range(len(X_train)):\n",
        "        counter += 1\n",
        "        if counter % 1000 == 0:\n",
        "          print (counter)\n",
        "        # Split the line by tabs\n",
        "\n",
        "        if Z_train[i] == 'sci-fi':\n",
        "            # Perform back translation\n",
        "            translated = translator.translate(X_train[i], src='en', dest='es').text\n",
        "            back_translated = translator.translate(translated, src='es', dest='en').text\n",
        "\n",
        "            # Perform synonym replacement\n",
        "            translated2 = translator.translate(X_train[i], src='en', dest='fr').text\n",
        "            back_translated2 = translator.translate(translated2, src='fr', dest='en').text\n",
        "\n",
        "            # Append plots\n",
        "            aug_X_train.append(X_train[i])\n",
        "            aug_Z_train.append(Z_train[i])\n",
        "            aug_X_train.append(back_translated)\n",
        "            aug_Z_train.append(Z_train[i])\n",
        "            aug_X_train.append(back_translated2)\n",
        "            aug_Z_train.append(Z_train[i])\n",
        "\n",
        "\n",
        "        elif Z_train[i] in {'crime', 'animation'}:\n",
        "            # Perform back translation\n",
        "            translated = translator.translate(X_train[i], src='en', dest='es').text\n",
        "            back_translated = translator.translate(translated, src='es', dest='en').text\n",
        "\n",
        "            # Append both plots to the specified file\n",
        "            aug_X_train.append(X_train[i])\n",
        "            aug_Z_train.append(Z_train[i])\n",
        "            aug_X_train.append(back_translated)\n",
        "            aug_Z_train.append(Z_train[i])\n",
        "\n",
        "\n",
        "        elif (Z_train[i] == 'drama'):\n",
        "            if (drama_counter < 1000):\n",
        "                drama_counter += 1\n",
        "                aug_X_train.append(X_train[i])\n",
        "                aug_Z_train.append(Z_train[i])\n",
        "\n",
        "        else:\n",
        "            aug_X_train.append(X_train[i])\n",
        "            aug_Z_train.append(Z_train[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufGZyt5MQnCF"
      },
      "source": [
        "# 4. Pre processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zb2p1HbDQ8W9"
      },
      "outputs": [],
      "source": [
        "#porter_stemmer=PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#Function to get WordNet POS tag\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun if no tag match\n",
        "\n",
        "#Function to lemmatize sentence with POS tagging\n",
        "def processSentence(s):\n",
        "    words = re.split(\"\\s+\", s.lower())  # Lowercasing and tokenizing\n",
        "    pos_tags = nltk.pos_tag(words)  # Get POS tags for each word\n",
        "    lemmed_words = [lemmatizer.lemmatize(w, get_wordnet_pos(pos)) for w, pos in pos_tags]\n",
        "    return ' '.join(lemmed_words)\n",
        "\n",
        "def filter_stopwords(tokens):\n",
        "    aux = [word for word in tokens if word.isalpha() and word not in stop_words] # watch out for isalpha()\n",
        "    return ' '.join(aux)\n",
        "\n",
        "#lemmatization and Lowercasing\n",
        "lemmed_train_plots = [processSentence(plot) for plot in aug_X_train]\n",
        "lemmed_test_plots = [processSentence(plot) for plot in X_test]\n",
        "lemmed_og_train_plots = [processSentence(plot) for plot in X_train]\n",
        "\n",
        "\n",
        "#Tokenizing\n",
        "tokenized_train_plots = [nltk.wordpunct_tokenize(sp) for sp in lemmed_train_plots]\n",
        "tokenized_test_plots = [nltk.wordpunct_tokenize(sp) for sp in lemmed_test_plots]\n",
        "tokenized_og_train_plots = [nltk.wordpunct_tokenize(sp) for sp in lemmed_og_train_plots]\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_train_plots = [filter_stopwords(tp) for tp in tokenized_train_plots]\n",
        "filtered_test_plots = [filter_stopwords(tp) for tp in tokenized_test_plots]\n",
        "filtered_og_train_plots = [filter_stopwords(tp) for tp in tokenized_og_train_plots]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY3n03mJRItx"
      },
      "source": [
        "# 5. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "KhvRXfkfRKUT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the Tokenizer\n",
        "tokenizer_og = Tokenizer(num_words=5000)  # Limit to top 5000 most frequent words\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(filtered_train_plots)\n",
        "tokenizer.fit_on_texts(filtered_og_train_plots)\n",
        "\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "X = tokenizer.texts_to_sequences(filtered_train_plots)\n",
        "X_og = tokenizer.texts_to_sequences(filtered_og_train_plots)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJpy38NcROzV"
      },
      "source": [
        "# 6. Padding Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "GN_pTU2xRYkx"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Pad the sequences (assuming max length of 100 words per plot)\n",
        "X = pad_sequences(X, maxlen=100)\n",
        "X_og = pad_sequences(X_og, maxlen=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7v6ShZWRcNU"
      },
      "source": [
        "# 7. Encoding the Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "j-GJWPqHRi7F"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Convert genres to numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(aug_Z_train)\n",
        "\n",
        "# Optionally convert to one-hot encoded format\n",
        "y = to_categorical(y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5siqFMW4RnEi"
      },
      "source": [
        "# 8. Building LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYyPKPm8RpA-",
        "outputId": "1f842983-6429-4444-c156-16e5b59d8c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model_og = Sequential()\n",
        "\n",
        "# Embedding layer (input_dim: vocabulary size, output_dim: embedding dimensions)\n",
        "model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
        "model_og.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
        "\n",
        "# LSTM layer (return_sequences=False for classification)\n",
        "model.add(LSTM(units=128))\n",
        "model_og.add(LSTM(units=128))\n",
        "\n",
        "\n",
        "# Optional Dropout layer to prevent overfitting\n",
        "model.add(Dropout(0.5))\n",
        "model_og.add(Dropout(0.5))\n",
        "\n",
        "# Fully connected layer for classification (adjust units for number of classes)\n",
        "model.add(Dense(units=9, activation='softmax'))  # Assuming 8 genres\n",
        "model_og.add(Dense(units=9, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_og.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRa4IOj4RwyG"
      },
      "source": [
        "# 9. Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "K_oZfnyjRyg1",
        "outputId": "cf1fbc08-a4e4-477b-b81b-036943fc7d8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 183ms/step - accuracy: 0.1910 - loss: 2.0678 - val_accuracy: 0.2192 - val_loss: 1.8716\n",
            "Epoch 2/3\n",
            "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 181ms/step - accuracy: 0.4084 - loss: 1.4841 - val_accuracy: 0.3969 - val_loss: 1.5632\n",
            "Epoch 3/3\n",
            "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 180ms/step - accuracy: 0.6687 - loss: 0.9126 - val_accuracy: 0.5183 - val_loss: 1.4423\n",
            "Epoch 1/3\n",
            "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 176ms/step - accuracy: 0.1762 - loss: 2.1833 - val_accuracy: 0.0000e+00 - val_loss: 2.2348\n",
            "Epoch 2/3\n",
            "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 181ms/step - accuracy: 0.1897 - loss: 2.1312 - val_accuracy: 0.0656 - val_loss: 2.2659\n",
            "Epoch 3/3\n",
            "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 179ms/step - accuracy: 0.4076 - loss: 1.7946 - val_accuracy: 0.0981 - val_loss: 2.4623\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step\n",
            "==== Pre processed ====\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'labels' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-90332999d4d0>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==== Pre processed ====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredicted_genre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Original\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(X, y, epochs=3, batch_size=32, validation_split=0.2)\n",
        "model_og.fit(X_og, y, epochs=3, batch_size=32, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre processed\n",
        "test_tokenized = tokenizer.texts_to_sequences(filtered_test_plots)\n",
        "test_padded = pad_sequences(test_tokenized, maxlen=100)\n",
        "\n",
        "labels = np.unique(genres).tolist()\n",
        "\n",
        "prediction = model.predict(test_padded)\n",
        "genre_index = prediction.argmax(axis=-1)\n",
        "\n",
        "predicted_genre = label_encoder.inverse_transform(genre_index)\n",
        "\n",
        "print(\"==== Pre processed ====\")\n",
        "\n",
        "print(classification_report(y_pred=predicted_genre, y_true = Z_test, labels = labels, zero_division=1.))\n",
        "\n",
        "# Original\n",
        "test_tokenized_og = tokenizer.texts_to_sequences(filtered_test_plots)\n",
        "test_padded_og = pad_sequences(test_tokenized_og, maxlen=100)\n",
        "\n",
        "\n",
        "prediction_og = model_og.predict(test_padded)\n",
        "genre_index_og = prediction_og.argmax(axis=-1)\n",
        "\n",
        "predicted_genre_og = label_encoder.inverse_transform(genre_index_og)\n",
        "\n",
        "print(\"==== Original ====\")\n",
        "\n",
        "print(classification_report(y_pred=predicted_genre_og, y_true = Z_test, labels = labels, zero_division=1.))\n"
      ],
      "metadata": {
        "id": "_iBYMd66LEEm",
        "outputId": "11fc057a-ec82-499d-898e-1d89890921b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n",
            "==== Pre processed ====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      action       0.45      0.45      0.45       113\n",
            "   animation       0.73      0.53      0.62        60\n",
            "      comedy       0.40      0.32      0.36       119\n",
            "       crime       0.35      0.42      0.38        57\n",
            "       drama       0.40      0.42      0.41       165\n",
            "      horror       0.53      0.74      0.62       101\n",
            "     romance       0.48      0.44      0.46        95\n",
            "      sci-fi       0.56      0.26      0.36        19\n",
            "     western       0.78      0.78      0.78        76\n",
            "\n",
            "    accuracy                           0.49       805\n",
            "   macro avg       0.52      0.49      0.49       805\n",
            "weighted avg       0.49      0.49      0.49       805\n",
            "\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step\n",
            "==== Original ====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      action       0.08      0.06      0.07       113\n",
            "   animation       0.03      0.03      0.03        60\n",
            "      comedy       0.15      0.20      0.17       119\n",
            "       crime       0.07      0.07      0.07        57\n",
            "       drama       0.21      0.30      0.25       165\n",
            "      horror       0.19      0.11      0.14       101\n",
            "     romance       0.04      0.03      0.04        95\n",
            "      sci-fi       0.02      0.05      0.03        19\n",
            "     western       0.00      0.00      0.00        76\n",
            "\n",
            "    accuracy                           0.13       805\n",
            "   macro avg       0.09      0.10      0.09       805\n",
            "weighted avg       0.11      0.13      0.12       805\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}